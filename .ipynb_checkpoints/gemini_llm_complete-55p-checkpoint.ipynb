{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0bf5e2b3-15fd-4fe8-aaa7-62ba4f7d9f4f",
   "metadata": {},
   "source": [
    "####  Google Gemini API Key Definition Along with Paths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098549ea-e961-462e-8102-609cf422ea7c",
   "metadata": {},
   "source": [
    "Key was defined for Google Gemini and saved to the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241ba643-a9b8-4816-9407-693ef654c5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"GEMINI_API_KEY\"]=\"AIzaSyBBF0rGyp8y0pJdfOnykCElApzNWJGuB7k\"\n",
    "chromadb_path = \"C:\\\\Users\\\\ASUS\\\\Desktop\\\\Python Deep Learning\\\\chroma_db\\\\\"\n",
    "chroma_collection_name =\"llm_rag_fellow\"\n",
    "pdfs_path = \"C:\\\\Users\\\\ASUS\\\\Desktop\\\\Python Deep Learning\\\\data\\\\articles\\\\\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07b7d6d-2a32-4c44-a946-8b030ad6c1b2",
   "metadata": {},
   "source": [
    "# Loading and Splitting The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181718e9-aef5-424f-93ee-c494ea6a9424",
   "metadata": {},
   "source": [
    "The data was collected from 55 PDFs one by one and saved as text variable. Then, it was split into chunks of 1000 characters each and prepared to create a local database with ChromaDB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f690089e-bc36-4fba-98b7-68ef810c1efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfReader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ccf6c7-1be2-4765-a479-1ab5d35379d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pdf_text(pdf_folder_path):\n",
    "    text = \"\"\n",
    "    for file_name in os.listdir(pdf_folder_path):\n",
    "        if file_name.endswith('.pdf'):\n",
    "            pdf_path = os.path.join(pdf_folder_path, file_name)\n",
    "            pdf_reader = PdfReader(pdf_path)\n",
    "            for page in pdf_reader.pages:\n",
    "                text += page.extract_text()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84e325c-7aa0-40e9-aa92-85144e6a067e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_folder_path = pdfs_path\n",
    "pdf_text = get_pdf_text(pdf_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e2e301-c117-48d1-bd18-fe5426b3be87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lenght of the chunked text\n",
    "print(len(pdf_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0242cc-48cf-4620-8eba-89949d40ec9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting the text into chunk\n",
    "import re\n",
    "def split_text(text: str):\n",
    "    split_text = re.split('\\n \\n',text)\n",
    "    return [i for i in split_text if i!=\"\"]\n",
    "\n",
    "chunked_text = split_text(text=pdf_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051a17d4-cfdf-4d24-a7f9-adb6c55df59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Length of chunked_text \n",
    "print(len(chunked_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f011294b-7e1f-40b2-b62d-d13c5e66ab3b",
   "metadata": {},
   "source": [
    "# Embedding Data [Vectorising]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379b9a6f-de0e-4289-b392-3d73d6b4d290",
   "metadata": {},
   "source": [
    "After splitting the data into chunks, we need to prepare it for storage in ChromaDB so that it can be easily retrieved later. To do this, we need to vectorize the data. Vectorization involves converting the textual data into numerical vectors, which can then be stored and processed efficiently. This process typically involves using techniques such as word embeddings or document embeddings to represent the text data in a high-dimensional space. Once the data is vectorized, it can be stored in ChromaDB, allowing for fast and efficient retrieval when needed. This vectorization step is crucial for ensuring that the data is stored in a format that is suitable for use with ChromaDB and other similar databases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7115202-e090-41db-bd18-e3d4166771b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "from chromadb import Documents, EmbeddingFunction, Embeddings\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7cfb824-8ebd-4913-ac89-cc342457b918",
   "metadata": {},
   "source": [
    "\r\n",
    "The Gemini embedding class has been imported since Gemini will be utilized as the LLM. This allows for seamless integration of Gemini's embedding functionality into the LLM framework, enabling efficient processing and analysis of text data. By incorporating Gemini embeddings, the LLM can leverage advanced semantic representations to enhance its understanding of textual information, leading to more accurate and insightful results in various natural language processing tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f995a62-4ec2-4af0-a262-131f747a9383",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeminiEmbeddingFunction(EmbeddingFunction):\n",
    "    def __call__(self, input:Documents)-> Embeddings:\n",
    "        gemini_api_key=os.getenv(\"GEMINI_API_KEY\")\n",
    "        if not gemini_api_key:\n",
    "            raise ValueError(\"API KEY NOT PROVIDED\")\n",
    "        genai.configure(api_key=gemini_api_key)\n",
    "        model = \"models/embedding-001\"\n",
    "        title = \"Custom query\"\n",
    "        return genai.embed_content(model = model,\n",
    "                                  content = input,\n",
    "                                  task_type=\"retrieval_document\",\n",
    "                                  title=title)[\"embedding\"]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9fe7c5-8899-4261-8468-fec3d8a084a3",
   "metadata": {},
   "source": [
    "# Storing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba0ac08-357f-47a7-84e5-df10b95ef881",
   "metadata": {},
   "source": [
    "The data has been saved within the specified path and with the defined name for later use with the LLM. In this way, the vectorized data, comprising the entire content of 55 articles, has been stored in the format required by ChromaDB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da2fb17-88f0-4a29-8a44-a1d89d7a87ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from typing import List\n",
    "def create_chroma_db(documents:List, path:str, name:str):\n",
    "\n",
    "    chroma_client = chromadb.PersistentClient(path=path)\n",
    "    db = chroma_client.create_collection(name=name,embedding_function=GeminiEmbeddingFunction())\n",
    "    for i, d in enumerate(documents):\n",
    "        db.add(documents=d, ids=str(i))\n",
    "    return db,name\n",
    "\n",
    "db, name=create_chroma_db(documents=chunked_text,\n",
    "                          path =chromadb_path,\n",
    "                          name=chroma_collection_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1004ad-aab4-4f9b-98c9-7e23b167c305",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_chroma_collection(path, name):\n",
    "\n",
    "    chroma_client = chromadb.PersistentClient(path=path)\n",
    "    db = chroma_client.get_collection(name=name, embedding_function=GeminiEmbeddingFunction())\n",
    "\n",
    "    return db\n",
    "\n",
    "db=load_chroma_collection(path=chromadb_path,\n",
    "                          name=chroma_collection_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eef00db-75af-4a05-a7d5-2d47dea518f0",
   "metadata": {},
   "source": [
    "# Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6d73d4-ac3c-4b09-ac5b-8573c3c78d2d",
   "metadata": {},
   "source": [
    "With this function, we were able to perform semantic search and obtain similar chunks within the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d320b0-e18d-4b71-a788-48a97a9cffe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relevant_passage(query, db, n_results):\n",
    "  passage = db.query(query_texts=[query], n_results=n_results)['documents'][0]\n",
    "  return passage\n",
    "\n",
    "#Example prompt\n",
    "relevant_text = get_relevant_passage(query=\"Can you analyze/enquire the pdf corpus to synthesize evidence on interlinkages between SDG 16 and the other two goals, SDG 1 + SDG 10?\",db=db,n_results=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5dffdc-cb8a-4135-98de-9511f3f83056",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(relevant_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f5b862-0c28-4d7d-845d-9b5f419b2544",
   "metadata": {},
   "source": [
    "We remind the LLM how it should return responses to us as prompts, ensuring that the answers will be in the desired format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41d8edb-d7a8-4c96-8d30-bfb2cd877b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_rag_prompt(query, relevant_passage):\n",
    "  escaped = relevant_passage.replace(\"'\", \"\").replace('\"', \"\").replace(\"\\n\", \" \")\n",
    "  prompt = (\"\"\"You are a helpful and informative bot that answers questions using text from the reference passage included below. \\\n",
    "  Be sure to respond in a complete sentence, being comprehensive, including all relevant background information. \\\n",
    "  You are talking to a technical audience make sure you give relevant text and passages\\\n",
    "  strike a formal and converstional tone. \\\n",
    "  If the passage is irrelevant to the answer, you may ignore it.\n",
    "  QUESTION: '{query}'\n",
    "  PASSAGE: '{relevant_passage}'\n",
    "\n",
    "  ANSWER:\n",
    "  \"\"\").format(query=query, relevant_passage=escaped)\n",
    "\n",
    "  return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4f48e4-911b-4847-96a8-ec4b538d8c0e",
   "metadata": {},
   "source": [
    "\n",
    "The provided function below is utilized to produce a response for a specified prompt using the Gemini Pro API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c915eb91-7e43-4bdf-996a-a872921f31f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "def generate_answer(prompt):\n",
    "    gemini_api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "    if not gemini_api_key:\n",
    "        raise ValueError(\"Gemini API Key not provided. Please provide GEMINI_API_KEY as an environment variable\")\n",
    "    genai.configure(api_key=gemini_api_key)\n",
    "    model = genai.GenerativeModel('gemini-pro')\n",
    "    answer = model.generate_content(prompt)\n",
    "    return answer.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16fe90f0-a891-49a9-b93d-3c37e98483d2",
   "metadata": {},
   "source": [
    "Testing the answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aca12fa-47e2-4235-81ba-612c41fd91f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_answer(\"How does increased accountability and increased transparency affect reducing poverty including relative and extreme poverty.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac550d7-dddb-491a-b321-dd4da59a9623",
   "metadata": {},
   "source": [
    "Retrieving the relevent 3 text chunks answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b91e4d-4ced-418d-98dc-b6e954c695d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#retrieve top 3 relevant text chunks and joining the relevant chunks to create a single passage\n",
    "def generate_answer_rel(db,query):\n",
    "    relevant_text = get_relevant_passage(query,db,n_results=3)\n",
    "    prompt = make_rag_prompt(query, \n",
    "                             relevant_passage=\"\".join(relevant_text))\n",
    "    answer = generate_answer(prompt)\n",
    "\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739179b5-463c-420d-b327-a1c6f50ef0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "db=load_chroma_collection(path=chromadb_path,\n",
    "                          name=chroma_collection_name)\n",
    "\n",
    "query=\"How does increased accountability and increased transparency affect reducing poverty including relative and extreme poverty. Hint: For accountability and increased transparency check SDG 16.6 and 16.7.Hint: For reducing poverty including relative and extreme poverty check SDG 1.1 and 1.2.\"\n",
    "\n",
    "answer = generate_answer_rel(db, query)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e2d11b-6947-4db6-a442-9c35c2c84447",
   "metadata": {},
   "source": [
    "Second question query "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d9e27b-718d-461e-9f50-188a587d8a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "query=\"Can you analyze/enquire the pdf corpus to synthesize evidence on interlinkages between SDG 16 and the other two goals, SDG 1 + SDG 10? Hint: You may use full text of the relevant SDG i.e., goal+targets+indicators for better matches.\"\n",
    "\n",
    "answer = generate_answer_rel(db, query)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d97a09-1b91-4e85-9acf-1e14bc927b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import session_info\n",
    "session_info.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f46712-791b-489d-9970-4900d5340a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import session_info\n",
    "\n",
    "# session_info modülünden kullanılan kütüphaneleri ve sürüm numaralarını alın\n",
    "requirements2 = session_info.show()\n",
    "\n",
    "# requirements.txt dosyasını oluşturun ve gereksinimleri yazın\n",
    "with open(\"requirements2.txt\", \"w\") as f:\n",
    "    for package, version in requirements2.items():\n",
    "        f.write(f\"{package}=={version}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22bd35a1-5b59-47ff-aeed-bfc374f665f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
